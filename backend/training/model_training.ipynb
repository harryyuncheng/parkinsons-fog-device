{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_RA7p8ykB4wP"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "q2UJeRtyI-QX"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqdYbSxOuNp"
      },
      "outputs": [],
      "source": [
        "# Set constants\n",
        "LEARNING_RATE = 0.004\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "SEQUENCE_LENGTH = 256\n",
        "DROPOUT_RATE = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "O4PEwwGtI-QY"
      },
      "outputs": [],
      "source": [
        "# Classes\n",
        "class IMUDataset(Dataset):\n",
        "    \"\"\"Dataset for IMU time series data with sliding windows\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.FloatTensor(sequences)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "class FOGClassifier(nn.Module):\n",
        "    \"\"\"CNN-LSTM model for FOG classification\"\"\"\n",
        "\n",
        "    def __init__(self, input_channels=6, sequence_length=SEQUENCE_LENGTH, num_classes=3, dropout_rate=DROPOUT_RATE):\n",
        "        super(FOGClassifier, self).__init__()\n",
        "\n",
        "        # 1D CNN layers for feature extraction\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, padding=3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.pool1 = nn.MaxPool1d(2) # Output length: sequence_length / 2\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool2 = nn.MaxPool1d(2) # Output length: (sequence_length / 2) / 2 = sequence_length / 4\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.pool3 = nn.MaxPool1d(2) # Output length: (sequence_length / 4) / 2 = sequence_length / 8\n",
        "\n",
        "        # Calculate the input size for the LSTM based on the sequence_length\n",
        "        lstm_input_size = 256 # Number of features from the last CNN layer\n",
        "        lstm_sequence_length = sequence_length // 8 # Length after 3 pooling layers\n",
        "\n",
        "        self.lstm = nn.LSTM(lstm_input_size, 128, batch_first=True, dropout=dropout_rate if dropout_rate > 0 else 0)\n",
        "\n",
        "        # Classification head\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: [batch_size, sequence_length, channels]\n",
        "        # Transpose for Conv1d: [batch_size, channels, sequence_length]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Transpose back for LSTM: [batch_size, sequence_length_after_pooling, features]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Use the last output for classification\n",
        "        x = lstm_out[:, -1, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        # Classification\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RNwq07wECBkn"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "def load_csv_data(data_dir='backend/data'):\n",
        "    \"\"\"Load all CSV files and combine them\"\"\"\n",
        "    csv_files = glob.glob(os.path.join(data_dir, \"session_*.csv\"))\n",
        "\n",
        "    if not csv_files:\n",
        "        print(f\"No CSV files found in {data_dir}\")\n",
        "        return None\n",
        "\n",
        "    all_data = []\n",
        "    print(f\"Loading {len(csv_files)} CSV files...\")\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            all_data.append(df)\n",
        "            print(f\"Loaded {csv_file}: {len(df)} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {csv_file}: {e}\")\n",
        "\n",
        "    if not all_data:\n",
        "        return None\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"Total combined data: {len(combined_df)} samples\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def create_sequences(data, sequence_length=SEQUENCE_LENGTH, overlap=0.5):\n",
        "    \"\"\"Create sliding window sequences from IMU data\"\"\"\n",
        "\n",
        "    # Extract features and labels\n",
        "    feature_cols = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
        "    X = data[feature_cols].values\n",
        "    y = data['label'].values\n",
        "\n",
        "    # Create label mapping\n",
        "    label_map = {'walking': 0, 'standing': 1, 'freezing': 2}\n",
        "    y_encoded = np.array([label_map.get(label, 1) for label in y])  # Default to standing if unknown\n",
        "\n",
        "    # Create sequences\n",
        "    step_size = int(sequence_length * (1 - overlap))\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(0, len(X) - sequence_length + 1, step_size):\n",
        "        sequence = X[i:i + sequence_length]\n",
        "        # Use the most common label in the window\n",
        "        window_labels = y_encoded[i:i + sequence_length]\n",
        "        most_common_label = np.bincount(window_labels).argmax()\n",
        "\n",
        "        sequences.append(sequence)\n",
        "        labels.append(most_common_label)\n",
        "\n",
        "    return np.array(sequences), np.array(labels), label_map\n",
        "\n",
        "def normalize_data(X_train, X_val, X_test):\n",
        "    \"\"\"Normalize the data using training set statistics\"\"\"\n",
        "    # Calculate mean and std across all features and time steps for training set\n",
        "    mean = np.mean(X_train, axis=(0, 1), keepdims=True)\n",
        "    std = np.std(X_train, axis=(0, 1), keepdims=True)\n",
        "    std = np.where(std == 0, 1, std)  # Avoid division by zero\n",
        "\n",
        "    # Normalize all sets\n",
        "    X_train_norm = (X_train - mean) / std\n",
        "    X_val_norm = (X_val - mean) / std\n",
        "    X_test_norm = (X_test - mean) / std\n",
        "\n",
        "    return X_train_norm, X_val_norm, X_test_norm, {'mean': mean, 'std': std}\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device='cpu'):\n",
        "    \"\"\"Train the model\"\"\"\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += batch_labels.size(0)\n",
        "            train_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_data, batch_labels in val_loader:\n",
        "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "                outputs = model(batch_data)\n",
        "                loss = criterion(outputs, batch_labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += batch_labels.size(0)\n",
        "                val_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch [{epoch}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f'Training completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "    return history\n",
        "\n",
        "def evaluate_model(model, test_loader, device='cpu'):\n",
        "    \"\"\"Evaluate the model on test data\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    return all_predictions, all_labels\n",
        "\n",
        "def plot_results(history, predictions, labels, label_map):\n",
        "    \"\"\"Plot training history and confusion matrix\"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training history\n",
        "    ax1.plot(history['train_loss'], label='Training Loss')\n",
        "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(history['train_acc'], label='Training Accuracy')\n",
        "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    ax2.set_title('Model Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    class_names = list(label_map.keys())\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names, ax=ax3)\n",
        "    ax3.set_title('Confusion Matrix')\n",
        "    ax3.set_xlabel('Predicted')\n",
        "    ax3.set_ylabel('Actual')\n",
        "\n",
        "    # Accuracy by class\n",
        "    accuracy_by_class = []\n",
        "    for i in range(len(class_names)):\n",
        "        if cm[i].sum() > 0:\n",
        "            accuracy_by_class.append(cm[i][i] / cm[i].sum())\n",
        "        else:\n",
        "            accuracy_by_class.append(0)\n",
        "\n",
        "    ax4.bar(class_names, accuracy_by_class)\n",
        "    ax4.set_title('Accuracy by Class')\n",
        "    ax4.set_ylabel('Accuracy')\n",
        "    ax4.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amP4lSUZI-QZ"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"FOG Classification Training Started\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load data\n",
        "    data = load_csv_data('data')\n",
        "    if data is None:\n",
        "        print(\"No data found. Make sure you have CSV files in backend/data/\")\n",
        "        return\n",
        "\n",
        "    # Print data statistics\n",
        "    print(f\"\\nData Statistics:\")\n",
        "    print(f\"Total samples: {len(data)}\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(data['label'].value_counts())\n",
        "\n",
        "    # Create sequences\n",
        "    sequence_length = SEQUENCE_LENGTH\n",
        "    overlap = 0.5\n",
        "    print(f\"\\nCreating sequences with length {sequence_length} and overlap {overlap}...\")\n",
        "    sequences, labels, label_map = create_sequences(data, sequence_length=sequence_length, overlap=overlap)\n",
        "    print(f\"Created {len(sequences)} sequences of length {sequence_length}\")\n",
        "    print(f\"Label mapping: {label_map}\")\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * len(sequences))\n",
        "    val_size = int(0.15 * len(sequences))\n",
        "    test_size = len(sequences) - train_size - val_size\n",
        "    X_train = sequences[:train_size]\n",
        "    y_train = labels[:train_size]\n",
        "    X_val = sequences[train_size:train_size + val_size]\n",
        "    y_val = labels[train_size:train_size + val_size]\n",
        "    X_test = sequences[train_size + val_size:]\n",
        "    y_test = labels[train_size + val_size:]\n",
        "\n",
        "    print(f\"\\nDataset split:\")\n",
        "    print(f\"Training: {len(X_train)} sequences\")\n",
        "    print(f\"Validation: {len(X_val)} sequences\")\n",
        "    print(f\"Test: {len(X_test)} sequences\")\n",
        "\n",
        "    # Normalize data\n",
        "    X_train_norm, X_val_norm, X_test_norm, norm_params = normalize_data(X_train, X_val, X_test)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = IMUDataset(X_train_norm, y_train)\n",
        "    val_dataset = IMUDataset(X_val_norm, y_val)\n",
        "    test_dataset = IMUDataset(X_test_norm, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    model = FOGClassifier(\n",
        "        input_channels=6,\n",
        "        sequence_length=sequence_length,\n",
        "        num_classes=3,\n",
        "        dropout_rate=DROPOUT_RATE\n",
        "    )\n",
        "\n",
        "    print(f\"\\nModel Architecture:\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Train model\n",
        "    print(f\"\\nStarting training...\")\n",
        "    history = train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(f\"\\nEvaluating on test set...\")\n",
        "    predictions, test_labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(test_labels, predictions, target_names=list(label_map.keys())))\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(history, predictions, test_labels, label_map)\n",
        "\n",
        "    # Save model\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    model_path = f'fog_classifier_{timestamp}.pth'\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'label_map': label_map,\n",
        "        'normalization_params': norm_params,\n",
        "        'model_config': {\n",
        "            'input_channels': 6,\n",
        "            'sequence_length': sequence_length,\n",
        "            'num_classes': 3,\n",
        "            'dropout_rate': DROPOUT_RATE\n",
        "        }\n",
        "    }, model_path)\n",
        "\n",
        "    print(f\"\\nModel saved as: {model_path}\")\n",
        "    print(f\"Training completed\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6CqBN0HsCPjV",
        "outputId": "d50f4c30-c50a-4ae4-9ba1-574ea91bfeec"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uU_AEBPiMRbx",
        "outputId": "f0fd61b2-58d2-4352-ad38-d9016c7951a6"
      },
      "outputs": [],
      "source": [
        "from torchview import draw_graph\n",
        "\n",
        "# Render graph\n",
        "batch_size = 1 \n",
        "input_channels = 6\n",
        "\n",
        "model = FOGClassifier(input_channels=input_channels, sequence_length=SEQUENCE_LENGTH, num_classes=3, dropout_rate=DROPOUT_RATE)\n",
        "dummy_input = torch.randn(batch_size, SEQUENCE_LENGTH, input_channels)\n",
        "model_graph = draw_graph(model, input_size=(batch_size, SEQUENCE_LENGTH, input_channels), expand_nested=False)\n",
        "model_graph.visual_graph"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
